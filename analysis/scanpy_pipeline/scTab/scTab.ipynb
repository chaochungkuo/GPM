{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Annotation of scRNA-seq\"\n",
    "author: \"Mohamed Mabrouk\"\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: false, input_fold: show}\n",
    "\n",
    "import tomlkit\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "\n",
    "from os import path\n",
    "import session_info\n",
    "import logging\n",
    "from tempfile import TemporaryDirectory\n",
    "from os import system\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6), frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "## Pipeline parameters\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = tomlkit.parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = config[\"basic\"][\"ANALYSIS_DIR\"]\n",
    "DIR_SAVE = path.join(ROOT_DIR, config[\"basic\"][\"DIR_SAVE\"])\n",
    "COUNTS_LAYER = config[\"normalization\"][\"COUNTS_LAYER\"]\n",
    "CLUSTERING_COL = config[\"clustering\"][\"CLUSTERING_COL\"]\n",
    "TISSUE = config[\"basic\"][\"TISSUE\"]\n",
    "ANNOTATION_METHOD = config[\"annotation\"][\"ANNOTATION_METHOD\"]\n",
    "NORMAMALIZATION_LAYER = config[\"normalization\"][\"NORMALIZATION_METHOD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make the download process more robust to errors\n",
    "def get_scTAB_resources():\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    resources_path = \"../resources/scTAB/\"\n",
    "\n",
    "    weights = hf_hub_download(\n",
    "        \"MohamedMabrouk/scTab\",\n",
    "        \"val_f1_macro_epoch=41_val_f1_macro=0.847.ckpt\",\n",
    "        local_dir=resources_path,\n",
    "    )\n",
    "\n",
    "    genes = hf_hub_download(\n",
    "        \"MohamedMabrouk/scTab\",\n",
    "        \"var.parquet\",\n",
    "        subfolder=\"merlin_cxg_2023_05_15_sf-log1p_minimal\",\n",
    "        local_dir=resources_path,\n",
    "    )\n",
    "\n",
    "    hyperparams = hf_hub_download(\n",
    "        \"MohamedMabrouk/scTab\", \"hparams.yaml\", local_dir=resources_path\n",
    "    )\n",
    "\n",
    "    cell_type = hf_hub_download(\n",
    "        \"MohamedMabrouk/scTab\",\n",
    "        \"cell_type.parquet\",\n",
    "        subfolder=\"merlin_cxg_2023_05_15_sf-log1p_minimal/categorical_lookup\",\n",
    "        local_dir=resources_path,\n",
    "    )\n",
    "\n",
    "    return weights, hyperparams, genes, cell_type\n",
    "\n",
    "\n",
    "def scTAB_data_loader(adata: AnnData, genes_path: str, batchsize: int = 2048):\n",
    "    from scipy.sparse import csc_matrix\n",
    "    from utils import streamline_count_matrix, dataloader_factory\n",
    "\n",
    "    genes_from_model = pd.read_parquet(genes_path)\n",
    "\n",
    "    # subset gene space only to genes used by the model\n",
    "    adata = adata[:, adata.var.index.isin(genes_from_model.feature_name)]\n",
    "    # pass the count matrix in csc_matrix to make column slicing efficient\n",
    "    x_streamlined = streamline_count_matrix(\n",
    "        csc_matrix(adata.X),\n",
    "        adata.var.index,  # change this if gene names are stored in different column\n",
    "        genes_from_model.feature_name,\n",
    "    )\n",
    "    loader = dataloader_factory(x_streamlined, batch_size=batchsize)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_scTAB_model(weights_path: str, hyperparams_path: str):\n",
    "    from collections import OrderedDict\n",
    "    import yaml\n",
    "    from utils import TabNet\n",
    "\n",
    "    # load checkpoint\n",
    "    if torch.cuda.is_available():\n",
    "        ckpt = torch.load(weights_path)\n",
    "    else:\n",
    "        # map to cpu if there is not gpu available\n",
    "        ckpt = torch.load(\n",
    "            weights_path,\n",
    "            map_location=torch.device(\"cpu\"),\n",
    "        )\n",
    "\n",
    "    # extract state_dict of tabnet model from checkpoint\n",
    "    # I can do this as well and just send you the updated checkpoint file - I think this would be the best solution\n",
    "    # I just put this here for completeness\n",
    "    tabnet_weights = OrderedDict()\n",
    "    for name, weight in ckpt[\"state_dict\"].items():\n",
    "        if \"classifier.\" in name:\n",
    "            tabnet_weights[name.replace(\"classifier.\", \"\")] = weight\n",
    "\n",
    "    with open(hyperparams_path) as f:\n",
    "        model_params = yaml.full_load(f.read())\n",
    "\n",
    "    # initialzie model with hparams from hparams.yaml file\n",
    "    tabnet = TabNet(\n",
    "        input_dim=model_params[\"gene_dim\"],\n",
    "        output_dim=model_params[\"type_dim\"],\n",
    "        n_d=model_params[\"n_d\"],\n",
    "        n_a=model_params[\"n_a\"],\n",
    "        n_steps=model_params[\"n_steps\"],\n",
    "        gamma=model_params[\"gamma\"],\n",
    "        n_independent=model_params[\"n_independent\"],\n",
    "        n_shared=model_params[\"n_shared\"],\n",
    "        epsilon=model_params[\"epsilon\"],\n",
    "        virtual_batch_size=model_params[\"virtual_batch_size\"],\n",
    "        momentum=model_params[\"momentum\"],\n",
    "        mask_type=model_params[\"mask_type\"],\n",
    "    )\n",
    "\n",
    "    # load trained weights\n",
    "    tabnet.load_state_dict(tabnet_weights)\n",
    "    # set model to inference mode\n",
    "    tabnet.eval()\n",
    "\n",
    "    return tabnet\n",
    "\n",
    "\n",
    "def sf_log1p_norm(x):\n",
    "    \"\"\"Normalize each cell to have 10000 counts and apply log(x+1) transform.\"\"\"\n",
    "\n",
    "    counts = torch.sum(x, dim=1, keepdim=True)\n",
    "    # avoid zero division error\n",
    "    counts += counts == 0.0\n",
    "    scaling_factor = 10000.0 / counts\n",
    "\n",
    "    return torch.log1p(scaling_factor * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 33131 × 36601\n",
       "    obs: 'sample', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_rb', 'log1p_total_counts_rb', 'pct_counts_rb', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'n_genes_by_counts_outlier', 'total_counts_outlier', 'pct_counts_mt_outlier', 'outlier', 'decontX_contamination', 'decontX_clusters', 'n_genes', 'doublet_score', 'predicted_doublet', 'S_score', 'G2M_score', 'phase', 'groups', 'leiden_0.2', 'leiden_0.4', 'leiden_0.6', 'leiden_0.8', 'leiden_1.0', 'leiden_1.2', 'leiden_1.4', 'leiden_1.6', 'leiden_1.8', 'leiden_2.0', 'leiden_2.2', 'leiden_2.4', 'leiden_2.6', 'leiden_2.8', 'cluster'\n",
       "    var: 'gene_ids', 'feature_types', 'mt', 'rb', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'mean', 'std', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    uns: 'FINAL_RESOLUTION', 'X_name', 'cluster', 'decontX', 'groups', 'hvg', 'leiden_0.2', 'leiden_0.2_colors', 'leiden_0.4', 'leiden_0.4_colors', 'leiden_0.6', 'leiden_0.6_colors', 'leiden_0.8', 'leiden_0.8_colors', 'leiden_1.0', 'leiden_1.0_colors', 'leiden_1.2', 'leiden_1.2_colors', 'leiden_1.4', 'leiden_1.4_colors', 'leiden_1.6', 'leiden_1.6_colors', 'leiden_1.8', 'leiden_1.8_colors', 'leiden_2.0', 'leiden_2.0_colors', 'leiden_2.2', 'leiden_2.2_colors', 'leiden_2.4', 'leiden_2.4_colors', 'leiden_2.6', 'leiden_2.6_colors', 'leiden_2.8', 'leiden_2.8_colors', 'log1p', 'neighbors', 'pca', 'sample_colors', 'scrublet', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'decontX_UMAP'\n",
       "    varm: 'PCs'\n",
       "    layers: 'counts', 'decontXcounts', 'log_norm'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = sc.read_h5ad(path.join(DIR_SAVE, \"adata.h5ad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223469/2841628016.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(weights_path)\n",
      "100%|██████████| 17/17 [00:12<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Getting a stable counts layer to be used later, setting X to be raw count values.\n",
    "if COUNTS_LAYER == \"X\":\n",
    "    adata.layers[\"counts\"] = adata.X.copy()\n",
    "    COUNTS_LAYER = \"counts\"\n",
    "elif COUNTS_LAYER in adata.layers.keys():\n",
    "    adata.X = adata.layers[COUNTS_LAYER].copy()\n",
    "else:\n",
    "    raise ValueError(\"{COUNTS_LAYER} layer can't be found in the object\")\n",
    "\n",
    "\n",
    "if ANNOTATION_METHOD == \"celltypist\":\n",
    "    exit(code=0)\n",
    "\n",
    "\n",
    "if ANNOTATION_METHOD == \"scGPT\":\n",
    "    print(\n",
    "        \"please use the accelerated_annotation notebook with a GPU, TPU, or HPU present.\"\n",
    "    )\n",
    "    exit(code=0)\n",
    "\n",
    "if ANNOTATION_METHOD == \"scTAB\":\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # BUG in cellnet\n",
    "    weights, hyperparams, genes, cell_type = get_scTAB_resources()\n",
    "    tabnet = get_scTAB_model(weights, hyperparams)\n",
    "    loader = scTAB_data_loader(adata, genes, batchsize=2048)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            # normalize data\n",
    "            x_input = sf_log1p_norm(batch[0][\"X\"])\n",
    "            logits, _ = tabnet(x_input)\n",
    "            preds.append(torch.argmax(logits, dim=1).numpy())\n",
    "\n",
    "    preds = np.hstack(preds)\n",
    "\n",
    "    cell_type_mapping = pd.read_parquet(cell_type)\n",
    "    preds = cell_type_mapping.loc[preds][\"label\"].to_numpy()\n",
    "    adata.obs[\"scTAB_label\"] = pd.Categorical(preds)\n",
    "    adata.write_h5ad(path.join(DIR_SAVE, \"adata.h5ad\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pixi - Python 3 (ipykernel)",
   "language": "python",
   "name": "pixi-kernel-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
