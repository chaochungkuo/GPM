{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"QC of scRNA-seq\"\n",
    "author: \"Mohamed Mabrouk\"\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "\n",
    "import tomlkit\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itables\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc_context\n",
    "import patchworklib as pw\n",
    "import seaborn.objects as so\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from os import walk, path, mkdir, listdir, cpu_count\n",
    "import session_info\n",
    "import logging\n",
    "from utils.preprocessing_funcs import *\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "sc.set_figure_params(dpi=100, dpi_save=300, format=\"png\")\n",
    "sc.settings.n_jobs = int(cpu_count() * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "\n",
    "\"in the following cell, override the default pipeline parameters if needed\"\n",
    "\n",
    "# QC Params\n",
    "CONCAT_SAMPLES: bool = True  # Concatenate all samples in one object, default: true\n",
    "CORRECT_AMBIENT_RNA: bool = (\n",
    "    False  # Correct ambient RNA, uses DecontX, Currently causes multiple erros.\n",
    ")\n",
    "FILTER_DOUBLETS: bool = False  # Filter doublets using Scrublet\n",
    "CELL_CYCLE_SCORE: bool = (\n",
    "    True  # Calculate cell cycle scores, based on scanpy implementation.\n",
    ")\n",
    "\n",
    "# TODO: Move to normalization\n",
    "# VARS_TO_REGRESS: List[str] = []     # list of regress (pct_counts_mt, pct_counts_ribo).\n",
    "# REGRESS: bool = False               # Regress out unwanted variables. Not recommended.\n",
    "\n",
    "\n",
    "############################################################\n",
    "####################     QC Dict      ######################\n",
    "############################################################\n",
    "\n",
    "\"\"\"\n",
    "Can either be a flat-dict for global threshold or a dict of dicts for each sample \n",
    "Entries in the form of col_name: [low, high] Ex: 'pct_counts_mt':[0, 20] which will be used as (low, high) pair.\n",
    "Also entries can be a single number Ex: 'pct_counts_mt': 3 which be used as number of MADS as follows media -+ nmad * MAD. \n",
    "\"\"\"\n",
    "\n",
    "# by default, filtering on read counts, number of features, and mitochondrial content\n",
    "qc_dict: Dict[str, List] | Dict[str, Number] = {\n",
    "    \"n_genes_by_counts\": 5,\n",
    "    \"total_counts\": 5,\n",
    "    \"pct_counts_mt\": 3,\n",
    "}\n",
    "\n",
    "# MODIFY_ME, Specifiy only after running diagnotics on the samples\n",
    "# global config\n",
    "# qc_dict: Dict[str, List] | Dict[str, List] = {\n",
    "#     \"pct_counts_mt\": [0, 10],\n",
    "#     \"pct_counts_rb\": [0, 10],\n",
    "#     \"n_genes_by_counts\": [1500, 9000],\n",
    "#     \"total_counts\": [3_000, 40_000]\n",
    "# }\n",
    "\n",
    "\n",
    "# # per-sample config\n",
    "# qc_dict: Dict[str, List] | Dict[str, List] = {\n",
    "#     \"sample1\": {\n",
    "#         \"pct_counts_mt\": [0, 10],\n",
    "#         \"pct_counts_rb\": [0, 10],\n",
    "#         \"total_counts\": [3_000, 40_000]\n",
    "#     },\n",
    "#     \"sample2\": {\n",
    "#         \"pct_counts_rb\": [0, 10],\n",
    "#         \"n_genes_by_counts\": [1500, 9000],\n",
    "#         \"total_counts\": [3_000, 40_000]\n",
    "#     },\n",
    "#     \"sample3\": {\n",
    "#         \"pct_counts_mt\": [0, 10],\n",
    "#         \"n_genes_by_counts\": [1500, 9000],\n",
    "#         \"total_counts\": [3_000, 40_000]\n",
    "#     },\n",
    "#     \"sample4\": {\n",
    "#         \"pct_counts_mt\": [0, 10],\n",
    "#         \"pct_counts_rb\": [0, 10],\n",
    "#         \"n_genes_by_counts\": [1500, 9000],\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "## Pipeline parameters\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = tomlkit.parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "\n",
    "# Directories\n",
    "ROOT_DIR = config[\"basic\"][\"ANALYSIS_DIR\"]\n",
    "DIR_SAVE = path.join(ROOT_DIR, config[\"basic\"][\"DIR_SAVE\"])\n",
    "DIR_samples = config[\"basic\"][\"DIR_SAMPLES\"]\n",
    "\n",
    "# Basic information\n",
    "TECHNOLOGY: str = config[\"basic\"][\"TECHNOLOGY\"]\n",
    "ORGANISM: str = config[\"basic\"][\"ORGANISM\"]\n",
    "AUTODISCOVER: bool = config[\"basic\"][\"auto_find\"]\n",
    "samples: Dict[str, str] = config[\"basic\"][\"samples\"]\n",
    "\n",
    "sample_components = inputs[TECHNOLOGY][\"files\"]\n",
    "black_list = inputs[TECHNOLOGY][\"black_list\"]\n",
    "read_function = inputs[TECHNOLOGY][\"function\"]\n",
    "raw_name = inputs[TECHNOLOGY][\"raw_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "# Keys\n",
    "\n",
    "keys = get_keys(qc_dict)\n",
    "\n",
    "if len(get_keys(qc_dict)) == 0:\n",
    "    raise ValueError(\"Couldn't get valid qc variables from QC dict.\")\n",
    "\n",
    "prior_keys = keys\n",
    "\n",
    "keys = keys + [\"pct_counts_rb\"]\n",
    "\n",
    "if FILTER_DOUBLETS and \"doublet_score\" not in keys:\n",
    "    keys = keys + [\"doublet_score\"]\n",
    "\n",
    "if CORRECT_AMBIENT_RNA and \"decontX_contamination\" not in keys:\n",
    "    keys = keys + [\"decontX_contamination\"]\n",
    "\n",
    "if CELL_CYCLE_SCORE and \"S_score\" not in keys:\n",
    "    keys = keys + [\"S_score\"]\n",
    "\n",
    "if CELL_CYCLE_SCORE and \"G2M_score\" not in keys:\n",
    "    keys += [\"G2M_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "\n",
    "# Diagnosic pipeline\n",
    "## Reading files\n",
    "\n",
    "if AUTODISCOVER and len(samples) == 0:\n",
    "    files = walk(DIR_samples)\n",
    "    for root, dir, files in files:\n",
    "        if (\n",
    "            len(set(sample_components).difference(set(files))) == 0\n",
    "            and path.basename(root) != raw_name\n",
    "        ):\n",
    "            samples[get_sample_name(root, black_list, 5)] = root\n",
    "else:\n",
    "    samples = config[\"basic\"][\"samples\"]\n",
    "    if len(samples) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No samples paths were provided, provide sample paths as a dictionary in 'config.toml'\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | warning: false\n",
    "itables.show(pd.DataFrame(samples, index=[\"sample path\"]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "adatas = {}\n",
    "raw_h5 = {}\n",
    "for sample_id, filename in samples.items():\n",
    "    sample_adata = read_function(filename)\n",
    "    sample_adata.var_names_make_unique()\n",
    "    adatas[sample_id] = sample_adata\n",
    "\n",
    "\n",
    "if TECHNOLOGY == \"10x\" and CORRECT_AMBIENT_RNA:\n",
    "    for sample_id, filename in samples.items():\n",
    "        files = listdir(path.dirname(filename))\n",
    "        raw_file = [\n",
    "            file for file in files if \"raw_feature_bc_matrix\" in file and \".h5\" in file\n",
    "        ]\n",
    "        if len(raw_file) == 1:\n",
    "            adata_raw = sc.read_10x_h5(path.join(path.dirname(filename), raw_file[0]))\n",
    "        else:\n",
    "            raise ValueError(\"No/Multiple raw files meeting condition were found\")\n",
    "\n",
    "        adata_raw.var_names_make_unique()\n",
    "        raw_h5[sample_id] = adata_raw\n",
    "\n",
    "\n",
    "if CONCAT_SAMPLES:\n",
    "    adata = ad.concat(adatas, label=\"sample\", join=\"outer\", merge=\"same\")\n",
    "    adata.obs_names_make_unique()\n",
    "    del samples\n",
    "\n",
    "if CONCAT_SAMPLES and TECHNOLOGY == \"10x\" and CORRECT_AMBIENT_RNA:\n",
    "    adata_raw = ad.concat(raw_h5, label=\"sample\", join=\"outer\", merge=\"same\")\n",
    "    adata_raw.obs_names_make_unique()\n",
    "    del raw_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "## Adding quality metrics\n",
    "mt_features = qc_features_fac[ORGANISM][\"mito\"]\n",
    "rb_features = qc_features_fac[ORGANISM][\"ribo\"]\n",
    "hb_features = qc_features_fac[ORGANISM][\"hb\"]\n",
    "\n",
    "\n",
    "# mitochondrial genes, \"MT-\" for human, \"Mt-\" for mouse\n",
    "adata.var[\"mt\"] = adata.var_names.str.startswith(tuple(mt_features))\n",
    "# ribosomal genes\n",
    "adata.var[\"rb\"] = adata.var_names.str.startswith(tuple(rb_features))\n",
    "# hemoglobin genes\n",
    "adata.var[\"hb\"] = adata.var_names.str.contains(\n",
    "    tuple(hb_features)[0]\n",
    ")  # Only regex is accepted\n",
    "\n",
    "sc.pp.calculate_qc_metrics(\n",
    "    adata, qc_vars=[\"mt\", \"rb\", \"hb\"], percent_top=[20], inplace=True, log1p=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "if all(map(lambda x: isinstance(x, (list, Number)), qc_dict.values())):\n",
    "    reduce_outliers(adata, qc_dict)\n",
    "elif all(map(lambda x: isinstance(x, dict), qc_dict.values())):\n",
    "    reduce_outliers(adata, qc_dict, subset=True)\n",
    "else:\n",
    "    raise ValueError(\"Please provide a QC-Dict in a valid format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "\n",
    "## Ambient RNA correction\n",
    "## TODO: Check if the Ambient RNA can be improved by using Batch information?\n",
    "\n",
    "if CORRECT_AMBIENT_RNA and TECHNOLOGY == \"10x\":\n",
    "\n",
    "    from os import system, remove, path\n",
    "    import tempfile\n",
    "    import urllib.request\n",
    "\n",
    "    with tempfile.TemporaryDirectory(dir=\".\") as tmpdirname:\n",
    "\n",
    "        # Workaround failure to install GenomeInfoDbData using pixi\n",
    "        dn_path = path.join(tmpdirname, \"GenomeInfoDbData_1.2.11.tar.gz\")\n",
    "        dn_url = \"https://bioconductor.org/packages/3.18/data/annotation/src/contrib/GenomeInfoDbData_1.2.11.tar.gz\"\n",
    "        urllib.request.urlretrieve(dn_url, filename=dn_path)\n",
    "        system(f\"R CMD INSTALL {dn_path}\")\n",
    "\n",
    "        # Define paths for temporary files\n",
    "        sce_path = path.join(tmpdirname, \"sce.h5ad\")\n",
    "        raw_path = path.join(tmpdirname, \"raw.h5ad\")\n",
    "        decontx_path = path.join(tmpdirname, \"decontX.h5ad\")\n",
    "\n",
    "        # Save adata and adata_raw to the temporary directory\n",
    "        adata.write_h5ad(sce_path)\n",
    "        adata_raw.write_h5ad(raw_path)\n",
    "\n",
    "        # Execute R scripts with temporary file paths\n",
    "        system(\n",
    "            f\"Rscript ./utils/deconx.R -s {sce_path} -r {raw_path} -o {decontx_path}\"\n",
    "        )\n",
    "\n",
    "        # Read the result back from the temporary directory\n",
    "        adata = sc.read_h5ad(decontx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "## Doublet Detection\n",
    "\n",
    "# TODO: Check real-life performance\n",
    "# TODO: Check Interop with R to convert object to R & vice-versa\n",
    "if FILTER_DOUBLETS and TECHNOLOGY == \"10x\":\n",
    "    sc.pp.scrublet(adata, batch_key=\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "## Cell cycle Scoring\n",
    "# **Not reliable, do via interop later**\n",
    "\n",
    "adata.layers[\"counts\"] = adata.X.copy()\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.scale(adata)\n",
    "\n",
    "\n",
    "if CELL_CYCLE_SCORE:\n",
    "    if ORGANISM in [\"human\", \"mouse\"]:\n",
    "\n",
    "        s_genes = [x.strip() for x in open(\"../resources/s_genes.txt\")]\n",
    "        g2m_genes = [x.strip() for x in open(\"../resources/s_genes.txt\")]\n",
    "\n",
    "        if ORGANISM == \"mouse\":\n",
    "            s_genes = human2mouse(s_genes)\n",
    "            g2m_genes = human2mouse(g2m_genes)\n",
    "\n",
    "        # Cell cycle scoring is not reliable and not similair to Seurat\n",
    "        sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes)\n",
    "    else:\n",
    "        logging.error(\"Organism must be either human or mouse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Plots (prior to filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic QC plots & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "nrows = len(keys) // ncols + len(keys) % ncols\n",
    "\n",
    "figsize = 3\n",
    "wspace = 0.5\n",
    "hspace = 0.5\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(\n",
    "        ncols * figsize + figsize * wspace * (ncols - 1),\n",
    "        nrows * figsize + hspace * (nrows - 1),\n",
    "    ),\n",
    ")\n",
    "plt.subplots_adjust(wspace=wspace, hspace=hspace)\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Prevent the subplots from showing\n",
    "plt.close(fig)\n",
    "\n",
    "for i, key in enumerate(keys):\n",
    "    sc.pl.violin(\n",
    "        adata, keys=[key], groupby=\"sample\", stripplot=False, inner=\"box\", ax=axs[i]\n",
    "    )\n",
    "    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if len(keys) < nrows * ncols:\n",
    "    for i in range(len(keys), nrows * ncols):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of basic QC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = adata.obs.groupby(\"sample\")[keys].agg([\"mean\", \"median\"]).round(3)\n",
    "df2 = adata.obs.groupby(\"sample\")[[\"sample\"]].agg([\"size\"])\n",
    "itables.show(pd.concat([df1, df2], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "\n",
    "from operator import add\n",
    "\n",
    "hvplot.extension(\"bokeh\")\n",
    "ls = []\n",
    "df = adata.obs[keys]\n",
    "for key in df.columns:\n",
    "    fig = (\n",
    "        adata.obs[[key, \"sample\"]]\n",
    "        .hvplot(kind=\"hist\", bins=300, width=400, line_color=\"#A3D5FF\", bgcolor=\"white\")\n",
    "        .opts(axiswise=True)\n",
    "    )\n",
    "    ls.append(fig)\n",
    "\n",
    "layout = reduce(add, ls)\n",
    "layout.cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown, display\n",
    "import io\n",
    "import base64\n",
    "import holoviews as hv\n",
    "\n",
    "n = len(keys)\n",
    "ncols = 2\n",
    "nrows = n // ncols + (n % ncols > 0)\n",
    "\n",
    "\n",
    "quarto_markdown = \"::: {.panel-tabset}\\n\\n\"\n",
    "\n",
    "\n",
    "# Create the subplots\n",
    "wspace = 0.5\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(\n",
    "        ncols * figsize + figsize * wspace * (ncols - 1),\n",
    "        nrows * figsize + wspace * (nrows - 1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(wspace=wspace, hspace=wspace)\n",
    "# Flatten the axes\n",
    "axs = axs.flatten()\n",
    "outlier_dict = {}\n",
    "\n",
    "# Plot a histogram on each subplot\n",
    "for i, col in enumerate(keys):\n",
    "    sns.histplot(data=df, x=col, ax=axs[i], bins=300)\n",
    "\n",
    "# Remove any unused subplots\n",
    "if len(df.columns) < nrows * ncols:\n",
    "    for i in range(len(df.columns), nrows * ncols):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "buf.seek(0)\n",
    "# Encode the image to base64\n",
    "img_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "# Add the image to the markdown\n",
    "quarto_markdown += (\n",
    "    f\"### All samples \\n\\n![All samples](data:image/png;base64,{img_str})\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "samples = adata.obs[\"sample\"].unique()\n",
    "samples = np.sort(samples)\n",
    "for sample in samples:\n",
    "    ls = []\n",
    "    df = adata.obs.loc[adata.obs[\"sample\"] == sample, keys]\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=nrows,\n",
    "        ncols=ncols,\n",
    "        figsize=(\n",
    "            ncols * figsize + figsize * wspace * (ncols - 1),\n",
    "            nrows * figsize + wspace * (nrows - 1),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.subplots_adjust(wspace=wspace, hspace=wspace)\n",
    "\n",
    "    # Flatten the axes\n",
    "    axs = axs.flatten()\n",
    "    outlier_dict = {}\n",
    "\n",
    "    # Plot a histogram on each subplot\n",
    "    for i, col in enumerate(keys):\n",
    "        sns.histplot(data=df, x=col, ax=axs[i], bins=300)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    if len(df.columns) < nrows * ncols:\n",
    "        for i in range(len(df.columns), nrows * ncols):\n",
    "            fig.delaxes(axs[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    # Encode the image to base64\n",
    "    img_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    # Add the image to the markdown\n",
    "    quarto_markdown += (\n",
    "        f\"### {sample} \\n\\n![{sample}](data:image/png;base64,{img_str})\\n\\n\"\n",
    "    )\n",
    "\n",
    "quarto_markdown += \":::\\n\"\n",
    "# Display the generated markdown\n",
    "display(Markdown(quarto_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots of confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "df = adata.obs[keys + [\"sample\"]]\n",
    "\n",
    "# Determine the number of rows and columns for your subplot grid\n",
    "n = len(df.columns)\n",
    "ncols = 2\n",
    "nrows = n // ncols\n",
    "figwidth = 5\n",
    "figheight = 4\n",
    "\n",
    "# Create the subplots\n",
    "wspace = 0.5\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(\n",
    "        ncols * figwidth + figwidth * wspace * (ncols - 1),\n",
    "        nrows * figheight + wspace * (nrows - 1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(wspace=wspace, hspace=wspace)\n",
    "\n",
    "# Flatten the axes\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.scatterplot(\n",
    "    df, x=\"total_counts\", y=\"n_genes_by_counts\", hue=\"sample\", alpha=0.4, s=6, ax=axs[0]\n",
    ")\n",
    "axs[0].legend(fancybox=True, framealpha=0.8)\n",
    "sns.scatterplot(\n",
    "    df, x=\"total_counts\", y=\"pct_counts_mt\", hue=\"sample\", alpha=0.4, s=6, ax=axs[1]\n",
    ")\n",
    "axs[1].legend(fancybox=True, framealpha=0.8)\n",
    "sns.scatterplot(\n",
    "    df, x=\"total_counts\", y=\"pct_counts_rb\", hue=\"sample\", alpha=0.4, s=6, ax=axs[2]\n",
    ")\n",
    "axs[2].legend(fancybox=True, framealpha=0.8)\n",
    "sns.move_legend(axs[0], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "sns.move_legend(axs[1], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "sns.move_legend(axs[2], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "if FILTER_DOUBLETS:\n",
    "    sns.scatterplot(\n",
    "        df, x=\"total_counts\", y=\"doublet_score\", hue=\"sample\", alpha=0.4, s=6, ax=axs[3]\n",
    "    )\n",
    "    sns.move_legend(axs[3], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "if CORRECT_AMBIENT_RNA:\n",
    "    sns.scatterplot(\n",
    "        df,\n",
    "        x=\"total_counts\",\n",
    "        y=\"decontX_contamination\",\n",
    "        hue=\"sample\",\n",
    "        alpha=0.4,\n",
    "        s=6,\n",
    "        ax=axs[4],\n",
    "    )\n",
    "    sns.move_legend(axs[4], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "[fig.delaxes(ax) for ax in axs.flatten() if not ax.has_data()]\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeconX contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "if CORRECT_AMBIENT_RNA and TECHNOLOGY == \"10x\":\n",
    "    ax1 = pw.Brick(figsize=(6, 6))\n",
    "    ax2 = pw.Brick(figsize=(6, 6))\n",
    "\n",
    "    scatter = sns.scatterplot(\n",
    "        pd.concat([adata.obsm[\"decontX_UMAP\"], adata.obs], axis=1),\n",
    "        x=\"DecontX_UMAP_1\",\n",
    "        y=\"DecontX_UMAP_2\",\n",
    "        hue=\"decontX_contamination\",\n",
    "        s=0.8,\n",
    "        ax=ax1,\n",
    "        palette=\"inferno\",\n",
    "    )\n",
    "    sns.move_legend(\n",
    "        scatter, \"center right\", bbox_to_anchor=(1.1, 0.5), title=None, frameon=False\n",
    "    )\n",
    "\n",
    "    scatter = sns.scatterplot(\n",
    "        pd.concat([adata.obsm[\"decontX_UMAP\"], adata.obs], axis=1),\n",
    "        x=\"DecontX_UMAP_1\",\n",
    "        y=\"DecontX_UMAP_2\",\n",
    "        hue=\"decontX_clusters\",\n",
    "        s=0.8,\n",
    "        ax=ax2,\n",
    "    )\n",
    "    sns.move_legend(\n",
    "        scatter, \"center right\", bbox_to_anchor=(1.15, 0.5), title=None, frameon=False\n",
    "    )\n",
    "\n",
    "    ax12 = ax1 + ax2\n",
    "    display(ax12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "sc.pp.pca(adata, n_comps=30)\n",
    "sc.pp.neighbors(adata)\n",
    "sc.tl.leiden(adata, key_added=\"groups\", flavor=\"leidenalg\")\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "figs = sc.pl.umap(\n",
    "    adata,\n",
    "    size=7,\n",
    "    color=[\"sample\"] + keys,\n",
    "    show=False,\n",
    "    ncols=2,\n",
    "    color_map=\"inferno\",\n",
    "    sort_order=False,\n",
    "    alpha=0.8,\n",
    "    hspace=0.2,\n",
    "    wspace=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "adata.obs.query(f\"{key}_outlier == False\").groupby(\"sample\").agg(\n",
    "    {\"pct_counts_mt\": \"mean\"}\n",
    ")\n",
    "ls = {}\n",
    "for key in prior_keys:\n",
    "    ls[key] = (\n",
    "        adata.obs.query(f\"{key}_outlier == False\").groupby(\"sample\")[key].agg(\"min\")\n",
    "    )\n",
    "min_df = pd.concat(ls, axis=1)\n",
    "\n",
    "for key in prior_keys:\n",
    "    ls[key] = (\n",
    "        adata.obs.query(f\"{key}_outlier == False\").groupby(\"sample\")[key].agg(\"max\")\n",
    "    )\n",
    "max_df = pd.concat(ls, axis=1)\n",
    "\n",
    "pd.concat([min_df, max_df], keys=[\"min\", \"max\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of outliers based on provided criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "# TODO: Fix\n",
    "df_l = []\n",
    "col_l = []\n",
    "for key in prior_keys:\n",
    "    try:\n",
    "        df_l.append(\n",
    "            pd.DataFrame(\n",
    "                adata.obs[[f\"{key}_outlier\", \"sample\"]].value_counts().loc[(True), :]\n",
    "            )\n",
    "        )\n",
    "        col_l.append(f\"{key}_outlier\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "col_l.append(f\"aggregate outliers\")\n",
    "df_l.append(\n",
    "    pd.DataFrame(adata.obs[[f\"outlier\", \"sample\"]].value_counts().loc[(True), :])\n",
    ")\n",
    "\n",
    "df_l.append(pd.DataFrame(adata.obs[[\"sample\"]].value_counts()))\n",
    "df_l[-1].index = df_l[-1].index.get_level_values(0)\n",
    "col_l.append(f\"Total Cells\")\n",
    "\n",
    "df = pd.concat(df_l, axis=1)\n",
    "df.columns = [col_l]\n",
    "df = df.loc[np.sort(df.index), :]\n",
    "itables.show(df.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell filtering based on outlier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "# Saving The object at the last step before subsseting\n",
    "# if path.exists(DIR_SAVE):\n",
    "#     adata.write_h5ad(path.join(DIR_SAVE, \"raw_adata.h5ad\"))\n",
    "# else:\n",
    "#     mkdir(DIR_SAVE)\n",
    "#     adata.write_h5ad(path.join(DIR_SAVE, \"raw_adata.h5ad\"))\n",
    "\n",
    "# Cell Filtering based on threshold\n",
    "\n",
    "\n",
    "adata = adata[(~adata.obs.outlier)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC plots (post filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of basic QC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "df1 = adata.obs.groupby(\"sample\")[keys].agg([\"mean\", \"median\"]).round(3)\n",
    "df2 = adata.obs.groupby(\"sample\")[[\"sample\"]].agg([\"size\"])\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown, display\n",
    "import io\n",
    "import base64\n",
    "import holoviews as hv\n",
    "\n",
    "df = adata.obs[keys]\n",
    "\n",
    "\n",
    "n = len(keys)\n",
    "ncols = 2\n",
    "nrows = n // ncols + (n % ncols > 0)\n",
    "\n",
    "\n",
    "quarto_markdown = \"::: {.panel-tabset}\\n\\n\"\n",
    "\n",
    "\n",
    "# Create the subplots\n",
    "wspace = 0.5\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(\n",
    "        ncols * figsize + figsize * wspace * (ncols - 1),\n",
    "        nrows * figsize + wspace * (nrows - 1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(wspace=wspace, hspace=wspace)\n",
    "# Flatten the axes\n",
    "axs = axs.flatten()\n",
    "outlier_dict = {}\n",
    "\n",
    "# Plot a histogram on each subplot\n",
    "for i, col in enumerate(keys):\n",
    "    sns.histplot(data=df, x=col, ax=axs[i], bins=300)\n",
    "\n",
    "# Remove any unused subplots\n",
    "if len(df.columns) < nrows * ncols:\n",
    "    for i in range(len(df.columns), nrows * ncols):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "buf.seek(0)\n",
    "# Encode the image to base64\n",
    "img_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "# Add the image to the markdown\n",
    "quarto_markdown += (\n",
    "    f\"### All samples \\n\\n![All samples](data:image/png;base64,{img_str})\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "samples = adata.obs[\"sample\"].unique()\n",
    "samples = np.sort(samples)\n",
    "for sample in samples:\n",
    "    ls = []\n",
    "    df = adata.obs.loc[adata.obs[\"sample\"] == sample, keys]\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=nrows,\n",
    "        ncols=ncols,\n",
    "        figsize=(\n",
    "            ncols * figsize + figsize * wspace * (ncols - 1),\n",
    "            nrows * figsize + wspace * (nrows - 1),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.subplots_adjust(wspace=wspace, hspace=wspace)\n",
    "\n",
    "    # Flatten the axes\n",
    "    axs = axs.flatten()\n",
    "    outlier_dict = {}\n",
    "\n",
    "    # Plot a histogram on each subplot\n",
    "    for i, col in enumerate(keys):\n",
    "        sns.histplot(data=df, x=col, ax=axs[i], bins=300)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    if len(df.columns) < nrows * ncols:\n",
    "        for i in range(len(df.columns), nrows * ncols):\n",
    "            fig.delaxes(axs[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    # Encode the image to base64\n",
    "    img_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    # Add the image to the markdown\n",
    "    quarto_markdown += (\n",
    "        f\"### {sample} \\n\\n![{sample}](data:image/png;base64,{img_str})\\n\\n\"\n",
    "    )\n",
    "\n",
    "quarto_markdown += \":::\\n\"\n",
    "# Display the generated markdown\n",
    "display(Markdown(quarto_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots of confounders after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "df = adata.obs[keys + [\"sample\"]]\n",
    "\n",
    "# Determine the number of rows and columns for your subplot grid\n",
    "n = len(df.columns)\n",
    "ncols = 2\n",
    "nrows = n // ncols\n",
    "figwidth = 5\n",
    "figheight = 4\n",
    "\n",
    "# Create the subplots\n",
    "wspace = 0.5\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(\n",
    "        ncols * figwidth + figwidth * wspace * (ncols - 1),\n",
    "        nrows * figheight + wspace * (nrows - 1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(wspace=wspace, hspace=wspace)\n",
    "\n",
    "# Flatten the axes\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.scatterplot(\n",
    "    df, x=\"total_counts\", y=\"n_genes_by_counts\", hue=\"sample\", alpha=0.4, s=6, ax=axs[0]\n",
    ")\n",
    "axs[0].legend(fancybox=True, framealpha=0.8)\n",
    "sns.scatterplot(\n",
    "    df, x=\"total_counts\", y=\"pct_counts_mt\", hue=\"sample\", alpha=0.4, s=6, ax=axs[1]\n",
    ")\n",
    "axs[1].legend(fancybox=True, framealpha=0.8)\n",
    "sns.scatterplot(\n",
    "    df, x=\"total_counts\", y=\"pct_counts_rb\", hue=\"sample\", alpha=0.4, s=6, ax=axs[2]\n",
    ")\n",
    "axs[2].legend(fancybox=True, framealpha=0.8)\n",
    "sns.move_legend(axs[0], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "sns.move_legend(axs[1], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "sns.move_legend(axs[2], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "if FILTER_DOUBLETS:\n",
    "    sns.scatterplot(\n",
    "        df, x=\"total_counts\", y=\"doublet_score\", hue=\"sample\", alpha=0.4, s=6, ax=axs[3]\n",
    "    )\n",
    "    sns.move_legend(axs[3], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "if CORRECT_AMBIENT_RNA:\n",
    "    sns.scatterplot(\n",
    "        df,\n",
    "        x=\"total_counts\",\n",
    "        y=\"decontX_contamination\",\n",
    "        hue=\"sample\",\n",
    "        alpha=0.4,\n",
    "        s=6,\n",
    "        ax=axs[4],\n",
    "    )\n",
    "    sns.move_legend(axs[4], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "[fig.delaxes(ax) for ax in axs.flatten() if not ax.has_data()]\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecontX contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "\n",
    "if CORRECT_AMBIENT_RNA and TECHNOLOGY == \"10x\":\n",
    "    ax1 = pw.Brick(figsize=(6, 6))\n",
    "    ax2 = pw.Brick(figsize=(6, 6))\n",
    "\n",
    "    scatter = sns.scatterplot(\n",
    "        pd.concat([adata.obsm[\"decontX_UMAP\"], adata.obs], axis=1),\n",
    "        x=\"DecontX_UMAP_1\",\n",
    "        y=\"DecontX_UMAP_2\",\n",
    "        hue=\"decontX_contamination\",\n",
    "        s=0.8,\n",
    "        ax=ax1,\n",
    "        palette=\"inferno\",\n",
    "    )\n",
    "    sns.move_legend(\n",
    "        scatter, \"center right\", bbox_to_anchor=(1.1, 0.5), title=None, frameon=False\n",
    "    )\n",
    "\n",
    "    scatter = sns.scatterplot(\n",
    "        pd.concat([adata.obsm[\"decontX_UMAP\"], adata.obs], axis=1),\n",
    "        x=\"DecontX_UMAP_1\",\n",
    "        y=\"DecontX_UMAP_2\",\n",
    "        hue=\"decontX_clusters\",\n",
    "        s=0.8,\n",
    "        ax=ax2,\n",
    "    )\n",
    "    sns.move_legend(\n",
    "        scatter, \"center right\", bbox_to_anchor=(1.15, 0.5), title=None, frameon=False\n",
    "    )\n",
    "\n",
    "    ax12 = ax1 + ax2\n",
    "    display(ax12)\n",
    "\n",
    "    # | echo: false\n",
    "# | warning: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CORRECT_AMBIENT_RNA and TECHNOLOGY == \"10x\":\n",
    "    with rc_context(\n",
    "        {\n",
    "            \"figure.figsize\": (\n",
    "                adata.obs[\"decontX_clusters\"].astype(\"int32\").max() * 0.7,\n",
    "                5,\n",
    "            )\n",
    "        }\n",
    "    ):\n",
    "        sns.violinplot(adata.obs, x=\"decontX_clusters\", y=\"decontX_contamination\")\n",
    "        sns.stripplot(\n",
    "            adata.obs, x=\"decontX_clusters\", y=\"decontX_contamination\", s=1, c=\"black\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering after to cell filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "sc.pp.pca(adata, n_comps=30)\n",
    "sc.pp.neighbors(adata)\n",
    "sc.tl.leiden(adata, key_added=\"groups\", flavor=\"leidenalg\")\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | warning: false\n",
    "with rc_context({\"figure.figsize\": (5, (len(keys) + 1) * 0.6)}):\n",
    "    figs = sc.pl.umap(\n",
    "        adata,\n",
    "        size=8,\n",
    "        color=[\"sample\"] + keys,\n",
    "        show=False,\n",
    "        ncols=2,\n",
    "        color_map=\"inferno\",\n",
    "        sort_order=False,\n",
    "        alpha=0.8,\n",
    "        hspace=0.2,\n",
    "        wspace=0.3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| echo: false\n",
    "# #| output: false\n",
    "# #| warning: false\n",
    "\n",
    "# ## Regression of Variables\n",
    "\n",
    "# # - [ ] Add error handling if the vars to regress is empty or contain non-keys\n",
    "# if REGRESS:\n",
    "#     sc.pp.regress_out(adata, keys= VARS_TO_REGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "## Save Result\n",
    "if path.exists(DIR_SAVE):\n",
    "    adata.write_h5ad(path.join(DIR_SAVE, \"adata.h5ad\"))\n",
    "else:\n",
    "    mkdir(DIR_SAVE)\n",
    "    adata.write_h5ad(path.join(DIR_SAVE, \"adata.h5ad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
