{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Annotation of scRNA-seq\"\n",
    "author: \"Mohamed Mabrouk\"\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: false, input_fold: show}\n",
    "\n",
    "import tomlkit\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "\n",
    "from os import path\n",
    "import session_info\n",
    "import logging\n",
    "from tempfile import TemporaryDirectory\n",
    "from os import system\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6), frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CELL_TYPIST model(s) to use\n",
    "CELL_TYPIST_MODELS: list[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "# | warning: false\n",
    "\n",
    "## Pipeline parameters\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = tomlkit.parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = config[\"basic\"][\"ANALYSIS_DIR\"]\n",
    "DIR_SAVE = path.join(ROOT_DIR, config[\"basic\"][\"DIR_SAVE\"])\n",
    "COUNTS_LAYER = config[\"normalization\"][\"COUNTS_LAYER\"]\n",
    "CLUSTERING_COL = config[\"clustering\"][\"CLUSTERING_COL\"]\n",
    "TISSUE = config[\"basic\"][\"TISSUE\"]\n",
    "ANNOTATION_METHOD = config[\"annotation\"][\"ANNOTATION_METHOD\"]\n",
    "NORMAMALIZATION_LAYER = config[\"normalization\"][\"NORMALIZATION_METHOD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_typist_annotate(adata: AnnData, models: list[str], inplace=True):\n",
    "    import celltypist\n",
    "    from celltypist import models as ctypist_models\n",
    "\n",
    "    if len(models) == 0:\n",
    "        raise ValueError(\"The models list are empty, enter valid model names.\")\n",
    "\n",
    "    all_models = ctypist_models.models_description().model.to_list()\n",
    "\n",
    "    for model in models:\n",
    "        if model not in all_models:\n",
    "            raise ValueError(\"{model} not found in supported cell typist models.\")\n",
    "\n",
    "    ctypist_models.download_models(force_update=True, model=models)\n",
    "\n",
    "    adata_celltypist = adata.copy()\n",
    "    adata_celltypist.X = adata.layers[COUNTS_LAYER]\n",
    "    sc.pp.normalize_per_cell(adata_celltypist, counts_per_cell_after=10**4)\n",
    "    sc.pp.log1p(adata_celltypist)\n",
    "    adata_celltypist.X = adata_celltypist.X.toarray()\n",
    "\n",
    "    for model in models:\n",
    "        loaded_model = ctypist_models.Model.load(model=model)\n",
    "        predictions = celltypist.annotate(\n",
    "            adata_celltypist, model=loaded_model, majority_voting=True\n",
    "        )\n",
    "        predictions_adata = predictions.to_adata()\n",
    "        adata.obs[\"celltypist_\" + model + \"_label\"] = predictions_adata.obs.loc[\n",
    "            adata.obs.index, \"majority_voting\"\n",
    "        ]\n",
    "        adata.obs[\"celltypist_\" + model + \"_conf_score\"] = predictions_adata.obs.loc[\n",
    "            adata.obs.index, \"conf_score\"\n",
    "        ]\n",
    "\n",
    "    if not inplace:\n",
    "        return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dn_scTAB_resources():\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    from os.path import exists\n",
    "    from os import system, makedirs\n",
    "\n",
    "    resources_path = \"../resources/scTAB/\"\n",
    "    check_point_file = \"scTab-checkpoints.tar.gz\"\n",
    "    check_point_directory = \"scTab-checkpoints\"\n",
    "    check_point_url = (\n",
    "        \"https://pklab.med.harvard.edu/felix/data/scTab-checkpoints.tar.gz\"\n",
    "    )\n",
    "    cxg_minimal_file = \"merlin_cxg_2023_05_15_sf-log1p_minimal.tar.gz\"\n",
    "    cxg_minimal_directory = \"merlin_cxg_2023_05_15_sf-log1p_minimal\"\n",
    "    cxg_minimual_url = \"https://pklab.med.harvard.edu/felix/data/merlin_cxg_2023_05_15_sf-log1p_minimal.tar.gz\"\n",
    "\n",
    "    if not exists(resources_path):\n",
    "        makedirs(resources_path)\n",
    "    if not exists(resources_path + check_point_file) and not exists(\n",
    "        resources_path + check_point_directory\n",
    "    ):\n",
    "        system(f\"wget {check_point_url} -o {check_point_file} -P {resources_path}\")\n",
    "    if not exists(resources_path + cxg_minimal_file) and not exists(\n",
    "        resources_path + cxg_minimal_directory\n",
    "    ):\n",
    "        system(f\"wget {cxg_minimual_url} -o {cxg_minimal_file} -P {resources_path}\")\n",
    "\n",
    "    if exists({resources_path + cxg_minimal_file}):\n",
    "        system(f\"tar -xzvf {resources_path+cxg_minimal_file}  -c {resources_path}\")\n",
    "        os.system(f\"rm {resources_path+cxg_minimal_file}\")\n",
    "\n",
    "    if exists({resources_path + check_point_file}):\n",
    "        system(f\"tar -xzvf {resources_path+check_point_file}  -c {resources_path}\")\n",
    "        system(f\"rm {resources_path+check_point_file}\")\n",
    "\n",
    "    genes_from_model = pd.read_parquet(\n",
    "        f\"{resources_path+cxg_minimal_directory}/var.parquet\"\n",
    "    )\n",
    "\n",
    "    return genes_from_model\n",
    "\n",
    "\n",
    "def scTAB_data_loader(\n",
    "    adata: AnnData, genes_from_model: DataFrame, batchsize: int = 2048\n",
    "):\n",
    "    from scipy.sparse import csc_matrix\n",
    "    from cellnet.utils.data_loading import streamline_count_matrix\n",
    "    from cellnet.utils.data_loading import dataloader_factory\n",
    "\n",
    "    # subset gene space only to genes used by the model\n",
    "    adata = adata[\n",
    "        :, adata.var.feature_name.isin(genes_from_model.feature_name).to_numpy()\n",
    "    ]\n",
    "    # pass the count matrix in csc_matrix to make column slicing efficient\n",
    "    x_streamlined = streamline_count_matrix(\n",
    "        csc_matrix(adata.X),\n",
    "        adata.var.feature_name,  # change this if gene names are stored in different column\n",
    "        genes_from_model.feature_name,\n",
    "    )\n",
    "    loader = dataloader_factory(x_streamlined, batch_size=batchsize)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_scTAB_model():\n",
    "    from collections import OrderedDict\n",
    "    import yaml\n",
    "    from cellnet.tabnet.tab_network import TabNet\n",
    "\n",
    "    # load checkpoint\n",
    "    if torch.cuda.is_available():\n",
    "        ckpt = torch.load(\n",
    "            \"../resources/scTAB/scTab-checkpoints/scTab/run5/val_f1_macro_epoch=41_val_f1_macro=0.847.ckpt\",\n",
    "        )\n",
    "    else:\n",
    "        # map to cpu if there is not gpu available\n",
    "        ckpt = torch.load(\n",
    "            \"../resources/scTABscTab-checkpoints/scTab/run5/val_f1_macro_epoch=41_val_f1_macro=0.847.ckpt\",\n",
    "            map_location=torch.device(\"cpu\"),\n",
    "        )\n",
    "\n",
    "        # extract state_dict of tabnet model from checkpoint\n",
    "        # I can do this as well and just send you the updated checkpoint file - I think this would be the best solution\n",
    "        # I just put this here for completeness\n",
    "        tabnet_weights = OrderedDict()\n",
    "        for name, weight in ckpt[\"state_dict\"].items():\n",
    "            if \"classifier.\" in name:\n",
    "                tabnet_weights[name.replace(\"classifier.\", \"\")] = weight\n",
    "\n",
    "    with open(\"../resources/scTab-checkpoints/scTab/run5/hparams.yaml\") as f:\n",
    "        model_params = yaml.full_load(f.read())\n",
    "\n",
    "    # initialzie model with hparams from hparams.yaml file\n",
    "    tabnet = TabNet(\n",
    "        input_dim=model_params[\"gene_dim\"],\n",
    "        output_dim=model_params[\"type_dim\"],\n",
    "        n_d=model_params[\"n_d\"],\n",
    "        n_a=model_params[\"n_a\"],\n",
    "        n_steps=model_params[\"n_steps\"],\n",
    "        gamma=model_params[\"gamma\"],\n",
    "        n_independent=model_params[\"n_independent\"],\n",
    "        n_shared=model_params[\"n_shared\"],\n",
    "        epsilon=model_params[\"epsilon\"],\n",
    "        virtual_batch_size=model_params[\"virtual_batch_size\"],\n",
    "        momentum=model_params[\"momentum\"],\n",
    "        mask_type=model_params[\"mask_type\"],\n",
    "    )\n",
    "\n",
    "    # load trained weights\n",
    "    tabnet.load_state_dict(tabnet_weights)\n",
    "    # set model to inference mode\n",
    "    tabnet.eval()\n",
    "\n",
    "    return tabnet\n",
    "\n",
    "\n",
    "def sf_log1p_norm(x):\n",
    "    \"\"\"Normalize each cell to have 10000 counts and apply log(x+1) transform.\"\"\"\n",
    "\n",
    "    counts = torch.sum(x, dim=1, keepdim=True)\n",
    "    # avoid zero division error\n",
    "    counts += counts == 0.0\n",
    "    scaling_factor = 10000.0 / counts\n",
    "\n",
    "    return torch.log1p(scaling_factor * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(path.join(DIR_SAVE, \"adata.h5ad\"))\n",
    "# adata = sc.read_h5ad(\"../save/marcelo_ref.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a stable counts layer to be used later, setting X to be raw count values.\n",
    "if COUNTS_LAYER == \"X\":\n",
    "    adata.layers[\"counts\"] = adata.X.copy()\n",
    "    COUNTS_LAYER = \"counts\"\n",
    "elif COUNTS_LAYER in adata.layers.keys():\n",
    "    adata.X = adata.layers[COUNTS_LAYER].copy()\n",
    "else:\n",
    "    raise ValueError(\"{COUNTS_LAYER} layer can't be found in the object\")\n",
    "\n",
    "\n",
    "if ANNOTATION_METHOD == \"celltypist\":\n",
    "    cell_typist_annotate(adata, CELL_TYPIST_MODELS)\n",
    "\n",
    "\n",
    "if ANNOTATION_METHOD == \"scGPT\":\n",
    "    print(\n",
    "        \"please use the accelerated_annotation notebook with a GPU, TPU, or HPU present.\"\n",
    "    )\n",
    "    exit(code=0)\n",
    "\n",
    "if ANNOTATION_METHOD == \"scTAB\":\n",
    "    import tqdm\n",
    "    from scipy.sparse import csc_matrix\n",
    "\n",
    "    genes_from_model = dn_scTAB_resources()\n",
    "    tabnet = get_scTAB_model()\n",
    "    loader = scTAB_data_loader(adata, genes_from_model, batchsize=2048)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            # normalize data\n",
    "            x_input = sf_log1p_norm(batch[0][\"X\"])\n",
    "            logits, _ = tabnet(x_input)\n",
    "            preds.append(torch.argmax(logits, dim=1).numpy())\n",
    "\n",
    "    preds = np.hstack(preds)\n",
    "    cell_type_mapping = pd.read_parquet(\n",
    "        \"../resources/scTAB/merlin_cxg_2023_05_15_sf-log1p_minimal/categorical_lookup/cell_type.parquet\"\n",
    "    )\n",
    "    preds = cell_type_mapping.loc[preds][\"label\"].to_numpy()\n",
    "    adata.obs[\"scTAB_label\"] = pd.Categorical(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.exists(\"../resources/scTAB/scTab-checkpoints.tar.gz\") and not os.path.exists(\n",
    "    \"../resources/scTAB/scTab-checkpoints.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from os import system, makedirs\n",
    "\n",
    "resources_path = \"../resources/scTAB/\"\n",
    "check_point_file = \"scTab-checkpoints.tar.gz\"\n",
    "check_point_directory = \"scTab-checkpoints\"\n",
    "check_point_url = \"https://pklab.med.harvard.edu/felix/data/scTab-checkpoints.tar.gz\"\n",
    "cxg_minimal_file = \"merlin_cxg_2023_05_15_sf-log1p_minimal.tar.gz\"\n",
    "cxg_minimal_directory = \"merlin_cxg_2023_05_15_sf-log1p_minimal\"\n",
    "cxg_minimual_url = \"https://pklab.med.harvard.edu/felix/data/merlin_cxg_2023_05_15_sf-log1p_minimal.tar.gz\"\n",
    "\n",
    "if not exists(resources_path):\n",
    "    makedirs(resources_path)\n",
    "if not exists(resources_path + check_point_file) and not exists(\n",
    "    resources_path + check_point_directory\n",
    "):\n",
    "    system(f\"wget {check_point_url} -o {check_point_file} -P {resources_path}\")\n",
    "if not exists(resources_path + cxg_minimal_file) and not exists(\n",
    "    resources_path + cxg_minimal_directory\n",
    "):\n",
    "    system(f\"wget {cxg_minimual_url} -o {cxg_minimal_file} -P {resources_path}\")\n",
    "\n",
    "if exists({resources_path + cxg_minimal_file}):\n",
    "    system(f\"tar -xzvf {resources_path+cxg_minimal_file}  -c {resources_path}\")\n",
    "    os.system(f\"rm {resources_path+cxg_minimal_file}\")\n",
    "\n",
    "if exists({resources_path + check_point_file}):\n",
    "    system(f\"tar -xzvf {resources_path+check_point_file}  -c {resources_path}\")\n",
    "    system(f\"rm {resources_path+check_point_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pixi - Python 3 (ipykernel)",
   "language": "python",
   "name": "pixi-kernel-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
